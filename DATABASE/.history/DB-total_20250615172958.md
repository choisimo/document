# 📚 Database Systems Comprehensive Guide
*A Complete Reference for Database Design, Implementation, and Management*

![Database Architecture](https://img.shields.io/badge/Database-Architecture-blue) ![SQL](https://img.shields.io/badge/SQL-Advanced-green) ![Transactions](https://img.shields.io/badge/Transactions-ACID-orange) ![Normalization](https://img.shields.io/badge/Normalization-BCNF%2F3NF-red)

---

## 📖 Table of Contents

1. [📊 Database Storage and Indexing](#database-storage-and-indexing)
2. [🏗️ Relational Database Design](#relational-database-design)
3. [🔐 Transaction Management](#transaction-management)
4. [🔧 Advanced SQL Operations](#advanced-sql-operations)
5. [🌐 Application Development](#application-development)
6. [⚙️ MySQL-Specific Features](#mysql-specific-features)
7. [📝 Summary Notes](#summary-notes)

---

## 🎯 Learning Objectives

By the end of this guide, you will be able to:

✅ Design efficient database storage structures using indexes and hashing  
✅ Apply normalization principles to create well-designed database schemas  
✅ Implement transaction management with ACID properties  
✅ Write complex SQL queries with joins, views, and constraints  
✅ Develop database-driven applications with proper security measures  
✅ Understand MySQL-specific syntax and optimization techniques  

---

## 📋 Document Overview

This comprehensive guide covers seven fundamental areas of database systems:

```mermaid
graph TD
    A[Database Systems] --> B[Storage & Indexing]
    A --> C[Database Design]
    A --> D[Transaction Management]
    A --> E[Advanced SQL]
    A --> F[Application Development]
    A --> G[MySQL Features]
    A --> H[Summary & Notes]
    
    B --> B1[Index Types]
    B --> B2[Hashing Techniques]
    B --> B3[B+ Trees]
    
    C --> C1[Normalization]
    C --> C2[Functional Dependencies]
    C --> C3[BCNF & 3NF]
    
    D --> D1[ACID Properties]
    D --> D2[Concurrency Control]
    D --> D3[Serializability]
    
    E --> E1[Joins]
    E --> E2[Views]
    E --> E3[Permissions]
    
    F --> F1[Web Applications]
    F --> F2[Security]
    F --> F3[Performance]
    
    G --> G1[Syntax Rules]
    G --> G2[Character Sets]
    G --> G3[Case Sensitivity]
```

---
# 📊 Database Storage and Indexing

> **"Efficient data retrieval is the foundation of high-performance database systems"**

## 🎯 Chapter Overview

Database indexing is like having a well-organized library catalog system. Instead of searching through every book on every shelf, you can use the catalog to directly locate the exact book you need. Similarly, database indexes allow rapid access to specific data without scanning entire tables.

```mermaid
graph LR
    A[Data Query] --> B{Index Available?}
    B -->|Yes| C[Use Index]
    B -->|No| D[Full Table Scan]
    C --> E[Direct Access]
    D --> F[Sequential Search]
    E --> G[Fast Result ⚡]
    F --> H[Slow Result 🐌]
    
    style C fill:#90EE90
    style D fill:#FFB6C1
    style G fill:#90EE90
    style H fill:#FFB6C1
```

---

## 🔑 Core Concepts

### 📍 Search Key
**Definition**: The attributes used to locate stored records in a database.

**Example**: In a student database, the student ID could serve as a search key to quickly find a specific student's information.

### 📂 Index File Structure
An index file is significantly smaller than the original data file and typically consists of:
- **Search key values**
- **Pointers to corresponding records**

```mermaid
graph TD
    A[Index File] --> B[Search Key 1]
    A --> C[Search Key 2]
    A --> D[Search Key N]
    B --> E[Pointer → Record 1]
    C --> F[Pointer → Record 2]
    D --> G[Pointer → Record N]
    
    style A fill:#ADD8E6
    style E fill:#98FB98
    style F fill:#98FB98
    style G fill:#98FB98
```

---

## 🏗️ Index Types

### 1️⃣ Ordered Index (순서 인덱스)

**Characteristics**:
- Records stored in sorted order by search key values
- Efficient for range queries and sorted data access
- Examples: "Find all employees with salary ≥ $50,000"

```mermaid
graph LR
    A[Index Entry 1] --> B[Index Entry 2]
    B --> C[Index Entry 3]
    C --> D[Index Entry 4]
    A1[Value: 100] --> A2[Pointer]
    B1[Value: 150] --> B2[Pointer]
    C1[Value: 200] --> C2[Pointer]
    D1[Value: 250] --> D2[Pointer]
    
    A --- A1
    B --- B1
    C --- C1
    D --- D1
```

#### 🔍 Primary Index (Clustering Index)
- **One per table** - The data file is physically ordered by the search key
- **Search key**: Usually (but not necessarily) the Primary Key
- **Physical storage**: Records are stored consecutively based on search key order

#### 🔍 Secondary Index (Non-clustering Index)
- **Multiple allowed** - Independent of physical storage order
- **Unique option**: Can be designated as secondary index even without being Primary Key
- **Foreign keys**: Can also be designated as indexes

### 2️⃣ Hash Index (해시 인덱스)

**Characteristics**:
- Uses hash functions to distribute data evenly across buckets
- Excellent for exact-match queries
- Examples: "Find employee with ID = 12345"

```mermaid
graph TD
    A[Search Key] --> B[Hash Function]
    B --> C[Bucket Address]
    C --> D[Data Records]
    
    E[Key: Music] --> F[h(Music) = 1]
    G[Key: History] --> H[h(History) = 2]
    I[Key: Physics] --> J[h(Physics) = 3]
    
    F --> K[Bucket 1]
    H --> L[Bucket 2]
    J --> M[Bucket 3]
    
    style B fill:#FFD700
    style K fill:#90EE90
    style L fill:#90EE90
    style M fill:#90EE90
```

---

## 📊 Index Performance Metrics

| Metric | Description | Optimization Goal |
|--------|-------------|-------------------|
| **Access Time** | Time to locate data | ⬇️ Minimize |
| **Insertion Time** | Time to insert new data + update index | ⬇️ Minimize |
| **Deletion Time** | Time to delete data + update index | ⬇️ Minimize |
| **Space Overhead** | Additional storage required for index | ⬇️ Minimize |

---

## 🎨 Dense vs Sparse Indexes

### 🟦 Dense Index (밀집 인덱스)

```mermaid
graph TD
    A[Dense Index] --> B[Index entry for EVERY search key]
    B --> C[Always Secondary Index]
    B --> D[Must use Indirect Reference]
    
    E[Record 1] --> F[Index Entry 1]
    G[Record 2] --> H[Index Entry 2]
    I[Record 3] --> J[Index Entry 3]
    K[Record 4] --> L[Index Entry 4]
    
    style A fill:#4169E1
    style C fill:#FFB6C1
    style D fill:#FFB6C1
```

**Characteristics**:
- Index entry exists for every search key value
- Always secondary index (unsorted)
- Must use indirect reference

### 🟨 Sparse Index (희소 인덱스)

```mermaid
graph TD
    A[Sparse Index] --> B[Index entries for SOME search keys only]
    B --> C[Only for Clustering Index]
    B --> D[Sequential search from nearest entry]
    
    E[Records 1-10] --> F[Index Entry 1]
    G[Records 11-20] --> H[Index Entry 2]
    I[Records 21-30] --> J[Index Entry 3]
    
    style A fill:#FFD700
    style C fill:#98FB98
    style D fill:#98FB98
```

**Characteristics**:
- Index entries exist for only some search key values
- Can only be used with clustering indexes
- Searches start from the nearest index entry and scan sequentially

---

## 🌳 B+ Trees

B+ Trees provide an excellent alternative to sequential indexing and are widely used in modern database systems.

### 📈 Advantages vs Disadvantages

```mermaid
graph LR
    A[B+ Trees] --> B[Advantages ✅]
    A --> C[Disadvantages ❌]
    
    B --> D[Minimal file reorganization]
    B --> E[Local changes only]
    B --> F[No full file reconstruction]
    B --> G[Balanced tree structure]
    
    C --> H[Insert/update overhead]
    C --> I[Additional storage space]
    
    style B fill:#90EE90
    style C fill:#FFB6C1
```

### 🏗️ B+ Tree Structure

**Key Properties**:
- **Balanced**: All paths from root to leaf have equal length
- **Node capacity**: Non-root nodes have n/2 to n children
- **Leaf capacity**: Leaf nodes contain (n-1)/2 to n-1 values

---

## 🔧 Hashing Techniques

### 🔨 Hash Functions

**Definition**: Functions that map arbitrary-length data to fixed-length data.

**Examples**: MD5, SHA-256

### 🪣 Bucket Management

```mermaid
graph TD
    A[Hash Function] --> B[Bucket 0]
    A --> C[Bucket 1]
    A --> D[Bucket 2]
    A --> E[Bucket N-1]
    
    F[Record 1] --> A
    G[Record 2] --> A
    H[Record 3] --> A
    
    style A fill:#FFD700
    style B fill:#ADD8E6
    style C fill:#ADD8E6
    style D fill:#ADD8E6
    style E fill:#ADD8E6
```

### ⚠️ Bucket Overflow

**Causes**:
1. **Skewed distribution**: Poor hash function causing uneven data distribution
2. **Duplicate keys**: Many records with the same search key value

**Solution**: **Overflow Chaining**
- Link overflow buckets using linked lists
- This structure is called **Closed Hashing**

### 🔄 Dynamic vs Static Hashing

| Aspect | Static Hashing | Dynamic Hashing |
|--------|----------------|-----------------|
| **Bucket Count** | Fixed | Variable |
| **Scalability** | Poor | Excellent |
| **Space Utilization** | May waste space | Efficient |
| **Reorganization** | Periodic full reorganization | Incremental adjustments |

---

## 🚀 Performance Optimization Guidelines

### 🎯 When to Use Each Index Type

```mermaid
graph TD
    A[Query Type?] --> B{Range Query?}
    A --> C{Exact Match?}
    A --> D{Storage Limited?}
    A --> E{Frequent Updates?}
    
    B -->|Yes| F[Use Ordered Index]
    C -->|Yes| G[Use Hash Index]
    D -->|Yes| H[Use Sparse Index]
    E -->|Yes| I[Minimize Index Count]
    
    style F fill:#90EE90
    style G fill:#90EE90
    style H fill:#FFD700
    style I fill:#FFB6C1
```

### 📋 Best Practices

1. **🎯 Choose appropriate index type** based on query patterns
2. **⚖️ Balance performance vs storage** overhead
3. **🔄 Regular maintenance** of indexes for optimal performance
4. **📊 Monitor query performance** and adjust indexing strategy accordingly

---

## 🌐 NoSQL and Alternative Storage

### 📊 NoSQL Overview

**Definition**: "Not Only SQL" - Non-relational database systems designed to overcome traditional relational database limitations.

```mermaid
graph TD
    A[NoSQL Models] --> B[Key-Value]
    A --> C[Document]
    A --> D[Column-Family]
    A --> E[Graph]
    
    B --> F[Simple, Scalable]
    C --> G[JSON-like structures]
    D --> H[Cassandra, HBase]
    E --> I[Relationship-focused]
    
    style A fill:#FF6347
    style B fill:#98FB98
    style C fill:#98FB98
    style D fill:#98FB98
    style E fill:#98FB98
```

**Popular Systems**: Cassandra, Hadoop, HBase, MongoDB

**Key Features**:
- **Schema-less**: Flexible data structures
- **Distributed processing**: Horizontal scalability
- **High availability**: Fault tolerance

---

## 🧪 Interactive Quiz

> **Test your understanding!**

**Question 1**: Which index type is best for the query "Find all employees with salary between $40,000 and $60,000"?

<details>
<summary>Click for answer</summary>

**Answer**: Ordered Index

**Explanation**: Range queries are most efficiently handled by ordered indexes because the data is stored in sorted order, allowing for efficient range scans.
</details>

**Question 2**: What is the main advantage of hash indexes over ordered indexes?

<details>
<summary>Click for answer</summary>

**Answer**: Faster exact-match queries

**Explanation**: Hash indexes can directly compute the bucket location for a specific key value, making exact-match queries very fast (O(1) average case).
</details>

---

## 🏗️ Relational Database Design

> **"Good database design is the foundation of data integrity and system efficiency"**

## 🎯 Chapter Overview

Database design is like architectural planning for a building. A well-designed database prevents structural problems, ensures efficient use of space, and provides a solid foundation for future expansion. Poor design leads to data redundancy, inconsistency, and maintenance nightmares.

```mermaid
graph TD
    A[Poor Schema Design] --> B[Data Redundancy]
    A --> C[Update Anomalies]
    A --> D[Insert Anomalies]
    A --> E[Delete Anomalies]
    
    F[Good Schema Design] --> G[Data Integrity]
    F --> H[Storage Efficiency]
    F --> I[Easy Maintenance]
    F --> J[Scalability]
    
    style A fill:#FFB6C1
    style B fill:#FFB6C1
    style C fill:#FFB6C1
    style D fill:#FFB6C1
    style E fill:#FFB6C1
    style F fill:#90EE90
    style G fill:#90EE90
    style H fill:#90EE90
    style I fill:#90EE90
    style J fill:#90EE90
```

---

## ⚠️ Database Design Problems

### 📊 The Three Anomalies

```mermaid
graph LR
    A[Database Anomalies] --> B[Insert Anomaly]
    A --> C[Update Anomaly]
    A --> D[Delete Anomaly]
    
    B --> B1[Cannot insert partial data]
    B --> B2[Forced NULL values]
    
    C --> C1[Data inconsistency]
    C --> C2[Multiple updates required]
    
    D --> D1[Unintended data loss]
    D --> D2[Information coupling]
    
    style A fill:#FF6347
    style B fill:#FFB6C1
    style C fill:#FFB6C1
    style D fill:#FFB6C1
```

#### 🚫 Insert Anomaly
**Problem**: Cannot insert certain data without having other unrelated data.

**Example**: Cannot add a new department to the system without having at least one instructor assigned to it.

#### 🔄 Update Anomaly
**Problem**: Same information stored in multiple places must be updated everywhere.

**Example**: If a department changes its budget, all instructor records referencing that department must be updated.

#### 🗑️ Delete Anomaly
**Problem**: Deleting one piece of information inadvertently removes other valuable information.

**Example**: Removing the last instructor from a department also removes all department information.

---

## 🔧 The Solution: Normalization

**Definition**: Normalization is the process of decomposing schemas to eliminate anomalies and ensure data integrity.

```mermaid
graph TD
    A[Unnormalized Schema] --> B[Identify Problems]
    B --> C[Apply Normalization Rules]
    C --> D[1NF: Atomic Values]
    D --> E[2NF: Remove Partial Dependencies]
    E --> F[3NF: Remove Transitive Dependencies]
    F --> G[BCNF: Remove All Anomalies]
    
    style A fill:#FFB6C1
    style G fill:#90EE90
```

---

## 🎯 Normal Forms

### 1️⃣ First Normal Form (1NF)

**Rule**: All attribute domains must be atomic (indivisible).

```mermaid
graph LR
    A[Non-Atomic Example] --> B[Student: John, Mary]
    A --> C[Phone: 123-456, 789-012]
    
    D[Atomic Example] --> E[Student: John]
    D --> F[Phone: 123-456]
    D --> G[Student: John]
    D --> H[Phone: 789-012]
    
    style A fill:#FFB6C1
    style B fill:#FFB6C1
    style C fill:#FFB6C1
    style D fill:#90EE90
    style E fill:#90EE90
    style F fill:#90EE90
    style G fill:#90EE90
    style H fill:#90EE90
```

**Examples of Non-Atomic Values**:
- Multiple phone numbers in one field: "123-456-7890, 987-654-3210"
- Composite addresses: "123 Main St, Apt 4B"
- Course codes with embedded meaning: "CS101" (CS = Computer Science, 101 = Level)

### 2️⃣ Functional Dependencies (FD)

**Definition**: An attribute or set of attributes X functionally determines another set of attributes Y if each X value is associated with exactly one Y value.

**Notation**: X → Y (X determines Y)

```mermaid
graph LR
    A[Student ID] --> B[Student Name]
    A --> C[Major]
    A --> D[GPA]
    
    E[Course ID] --> F[Course Name]
    E --> G[Credits]
    E --> H[Department]
    
    style A fill:#FFD700
    style E fill:#FFD700
```

#### 🔍 Armstrong's Axioms

**Three fundamental rules for deriving functional dependencies**:

1. **Reflexivity**: If Y ⊆ X, then X → Y
2. **Augmentation**: If X → Y, then XZ → YZ
3. **Transitivity**: If X → Y and Y → Z, then X → Z

```mermaid
graph TD
    A[Armstrong's Axioms] --> B[Reflexivity]
    A --> C[Augmentation]
    A --> D[Transitivity]
    
    B --> B1[If Y ⊆ X, then X → Y]
    C --> C1[If X → Y, then XZ → YZ]
    D --> D1[If X → Y and Y → Z, then X → Z]
    
    style A fill:#4169E1
    style B1 fill:#ADD8E6
    style C1 fill:#ADD8E6
    style D1 fill:#ADD8E6
```

#### 🔄 Closure Computation

**Algorithm to compute attribute closure (α⁺)**:

```
result := α
while (result changes) do
    for each β → γ in F do
        if β ⊆ result then
            result := result ∪ γ
```

**Example**: Given F = {A → B, C → D, A → C} and α = {A}

```mermaid
graph TD
    A[Start: result = {A}] --> B[Apply A → B: result = {A,B}]
    B --> C[Apply A → C: result = {A,B,C}]
    C --> D[Apply C → D: result = {A,B,C,D}]
    D --> E[No more changes: {A}⁺ = {A,B,C,D}]
    
    style A fill:#FFD700
    style E fill:#90EE90
```

---

## 🏆 Advanced Normal Forms

### 3️⃣ Third Normal Form (3NF)

**Definition**: A relation is in 3NF if, for every functional dependency α → β:
1. α → β is trivial (β ⊆ α), OR
2. α is a superkey, OR  
3. Each attribute in β - α is part of some candidate key

```mermaid
graph TD
    A[3NF Conditions] --> B[Trivial FD]
    A --> C[Superkey Determinant]
    A --> D[Prime Attribute]
    
    B --> B1[β ⊆ α]
    C --> C1[α is superkey]
    D --> D1[β-α attributes are prime]
    
    style A fill:#4169E1
    style B1 fill:#90EE90
    style C1 fill:#90EE90
    style D1 fill:#FFD700
```

**Advantages**:
✅ Allows some redundancy for dependency preservation  
✅ Can check dependencies without joins  
✅ Guarantees lossless decomposition  

### 4️⃣ Boyce-Codd Normal Form (BCNF)

**Definition**: A relation is in BCNF if, for every functional dependency α → β:
1. α → β is trivial (β ⊆ α), OR
2. α is a superkey

```mermaid
graph TD
    A[BCNF Conditions] --> B[Trivial FD]
    A --> C[Superkey Determinant]
    
    B --> B1[β ⊆ α]
    C --> C1[α is superkey]
    
    D[BCNF vs 3NF] --> E[BCNF: Stricter]
    D --> F[3NF: More Permissive]
    
    E --> G[Eliminates all anomalies]
    F --> H[May preserve dependencies]
    
    style A fill:#4169E1
    style E fill:#FF6347
    style F fill:#FFD700
    style G fill:#90EE90
    style H fill:#90EE90
```

---

## 🔄 Decomposition Process

### 📋 BCNF Decomposition Algorithm

```mermaid
graph TD
    A[Start with Relation R] --> B{BCNF Violation?}
    B -->|No| C[Done: R is in BCNF]
    B -->|Yes| D[Find violating FD: α → β]
    D --> E[Create R1 = α ∪ β]
    E --> F[Create R2 = R - (β - α)]
    F --> G[Recursively check R1, R2]
    G --> B
    
    style C fill:#90EE90
    style D fill:#FFB6C1
```

**Example Decomposition**:

**Original**: Student_Course(Student_ID, Course_Name, Instructor)  
**FDs**: {Student_ID, Course_Name} → Instructor, Instructor → Course_Name  

**Problem**: Instructor → Course_Name violates BCNF (Instructor is not a superkey)

**Solution**:
- R1(Instructor, Course_Name)
- R2(Student_ID, Instructor)

### ⚖️ Design Goals

```mermaid
graph TD
    A[Decomposition Goals] --> B[Lossless Decomposition]
    A --> C[Dependency Preservation]
    A --> D[Eliminate Anomalies]
    
    B --> B1[R = R1 ⋈ R2 ⋈ ... ⋈ Rn]
    C --> C1[All FDs can be checked locally]
    D --> D1[No insert/update/delete anomalies]
    
    E[Trade-offs] --> F[BCNF + Lossless]
    E --> G[3NF + Dependency Preservation]
    
    style B1 fill:#90EE90
    style C1 fill:#FFD700
    style D1 fill:#90EE90
    style F fill:#FF6347
    style G fill:#4169E1
```

---

## 🎮 Hands-On Exercise

### 🧩 Normalization Challenge

**Given Relation**: Employee_Project(EmpID, EmpName, ProjID, ProjName, Hours, DeptName, DeptBudget)

**Functional Dependencies**:
- EmpID → EmpName, DeptName, DeptBudget
- ProjID → ProjName  
- EmpID, ProjID → Hours

**Tasks**:
1. Identify the current normal form
2. Decompose to BCNF
3. Check for lossless decomposition

<details>
<summary>💡 Solution</summary>

**Current Form**: 1NF (has update anomalies due to transitive dependencies)

**BCNF Decomposition**:
1. **Employee**(EmpID, EmpName, DeptName, DeptBudget)
2. **Project**(ProjID, ProjName)  
3. **Assignment**(EmpID, ProjID, Hours)

**Verification**: Lossless because:
- Employee ∩ Assignment = {EmpID} and EmpID → EmpName, DeptName, DeptBudget
- Project ∩ Assignment = {ProjID} and ProjID → ProjName
</details>

---

## 🔍 Advanced Topics

### 🔄 Denormalization Considerations

Sometimes, **strategic denormalization** can improve performance:

```mermaid
graph LR
    A[Normalized Design] --> B[Multiple Joins]
    B --> C[Query Complexity]
    
    D[Denormalized Design] --> E[Fewer Joins]
    E --> F[Faster Queries]
    E --> G[Storage Redundancy]
    
    style A fill:#90EE90
    style D fill:#FFD700
    style C fill:#FFB6C1
    style F fill:#90EE90
    style G fill:#FFB6C1
```

**When to Consider Denormalization**:
- **Read-heavy workloads** with infrequent updates
- **Performance-critical queries** requiring multiple joins
- **Data warehousing** scenarios

### 🪟 Materialized Views

**Alternative to denormalization**: Precomputed query results stored as tables

**Advantages**:
✅ Maintains normalized base tables  
✅ Improves query performance  
✅ Automatic maintenance (in some systems)  

**Trade-offs**:
❌ Additional storage overhead  
❌ Maintenance cost on updates  
❌ Potential staleness issues  

---

# 🔐 Transaction Management

> **"In the world of databases, transactions are the guardians of data integrity"**

## 🎯 Chapter Overview

Imagine multiple people trying to withdraw money from the same bank account simultaneously, or two customers trying to book the last available airline seat. Without proper transaction management, chaos would ensue! Transactions ensure that database operations are reliable, consistent, and predictable even in concurrent, failure-prone environments.

```mermaid
graph TD
    A[Why Transactions?] --> B[Concurrent Access]
    A --> C[System Failures]
    A --> D[Data Integrity]
    
    B --> B1[Multiple users, same data]
    B --> B2[Race conditions]
    B --> B3[Lost updates]
    
    C --> C1[Hardware failures]
    C --> C2[Software crashes]
    C --> C3[Network issues]
    
    D --> D1[Business rules]
    D --> D2[Consistency constraints]
    D --> D3[Referential integrity]
    
    style A fill:#FF6347
    style B fill:#FFB6C1
    style C fill:#FFB6C1
    style D fill:#90EE90
```

### 🎓 Learning Objectives

By the end of this section, you will be able to:

✅ **Define** transactions and explain their importance  
✅ **Apply** ACID properties to real-world scenarios  
✅ **Analyze** concurrency problems and their solutions  
✅ **Implement** transaction control in SQL  
✅ **Design** recovery strategies for system failures  
✅ **Evaluate** different isolation levels and their trade-offs  

---

## 🏗️ What is a Transaction?

**Definition**: A transaction is a logical unit of work that contains one or more SQL statements. All statements in a transaction are executed as a single, indivisible unit.

### 🌟 Real-World Example: Bank Transfer

```mermaid
sequenceDiagram
    participant User
    participant Database
    participant Account_A
    participant Account_B
    
    User->>Database: BEGIN TRANSACTION
    User->>Account_A: Debit $100
    Account_A-->>Database: Balance: $500 → $400
    User->>Account_B: Credit $100
    Account_B-->>Database: Balance: $300 → $400
    User->>Database: COMMIT
    
    Note over Database: Transaction Successful ✅
```

**Transaction Steps**:
1. **BEGIN** - Start the transaction
2. **Debit from Account A** - Subtract $100
3. **Credit to Account B** - Add $100  
4. **COMMIT** - Make changes permanent

**What if something goes wrong?**

```mermaid
sequenceDiagram
    participant User
    participant Database
    participant Account_A
    participant Account_B
    
    User->>Database: BEGIN TRANSACTION
    User->>Account_A: Debit $100
    Account_A-->>Database: Balance: $500 → $400
    User->>Account_B: Credit $100
    Note over Account_B: ❌ SYSTEM FAILURE
    User->>Database: ROLLBACK
    Account_A-->>Database: Balance: $400 → $500
    
    Note over Database: Transaction Rolled Back 🔄
```

---

## 🏛️ ACID Properties

The cornerstone of transaction management - **ACID** properties ensure database reliability:

```mermaid
graph TD
    A[ACID Properties] --> B[Atomicity 🎯]
    A --> C[Consistency 🔄]
    A --> D[Isolation 🏝️]
    A --> E[Durability 💾]
    
    B --> B1[All or Nothing]
    B --> B2[No partial execution]
    
    C --> C1[Valid state transitions]
    C --> C2[Integrity constraints]
    
    D --> D1[Concurrent isolation]
    D --> D2[No interference]
    
    E --> E1[Permanent changes]
    E --> E2[Survive failures]
    
    style A fill:#4169E1
    style B fill:#FF6347
    style C fill:#32CD32
    style D fill:#FFD700
    style E fill:#9370DB
```

### 🎯 Atomicity (원자성)

**Principle**: "All or Nothing" - Either all operations in a transaction complete successfully, or none of them do.

```mermaid
graph LR
    A[Transaction T1] --> B{All Operations Successful?}
    B -->|Yes| C[COMMIT - All Changes Applied]
    B -->|No| D[ROLLBACK - No Changes Applied]
    
    E[Operation 1: ✅] --> F[Operation 2: ✅]
    F --> G[Operation 3: ✅]
    G --> H[Transaction Succeeds]
    
    I[Operation 1: ✅] --> J[Operation 2: ❌]
    J --> K[Entire Transaction Fails]
    
    style C fill:#90EE90
    style D fill:#FFB6C1
    style H fill:#90EE90
    style K fill:#FFB6C1
```

**Implementation Mechanisms**:
- **Write-Ahead Logging (WAL)**: Log changes before applying them
- **Shadow Paging**: Keep original pages until commit
- **Rollback Segments**: Store undo information

### 🔄 Consistency (일관성)

**Principle**: Transactions move the database from one consistent state to another consistent state.

```mermaid
graph TD
    A[Consistent State 1] --> B[Transaction Execution]
    B --> C[Consistent State 2]
    
    D[Example: Bank Database] --> E[Total Money = $1000]
    E --> F[Transfer $100: A→B]
    F --> G[Total Money = $1000]
    
    H[Integrity Constraints] --> I[Primary Keys]
    H --> J[Foreign Keys]
    H --> K[Check Constraints]
    H --> L[Business Rules]
    
    style A fill:#90EE90
    style C fill:#90EE90
    style E fill:#90EE90
    style G fill:#90EE90
```

**Types of Consistency**:
- **Entity Integrity**: Primary key constraints
- **Referential Integrity**: Foreign key constraints  
- **Domain Integrity**: Data type and check constraints
- **User-Defined**: Business rule constraints

### 🏝️ Isolation (고립성)

**Principle**: Concurrent transactions should not interfere with each other. Each transaction should appear to execute in isolation.

```mermaid
graph TD
    A[Multiple Concurrent Transactions] --> B[T1: Transfer Money]
    A --> C[T2: Calculate Interest]
    A --> D[T3: Generate Report]
    
    B --> E[Sees Consistent Snapshot]
    C --> F[Sees Consistent Snapshot]
    D --> G[Sees Consistent Snapshot]
    
    E --> H[No Interference]
    F --> H
    G --> H
    
    style H fill:#90EE90
```

**Concurrency Problems Without Isolation**:
- **Dirty Read**: Reading uncommitted changes
- **Non-Repeatable Read**: Same query returns different results
- **Phantom Read**: New rows appear in range queries
- **Lost Update**: Concurrent updates overwrite each other

### 💾 Durability (지속성)

**Principle**: Once a transaction commits, its changes are permanent and survive system failures.

```mermaid
graph TD
    A[Transaction Commits] --> B[Changes Written to Disk]
    B --> C[System Failure Occurs]
    C --> D[System Restarts]
    D --> E[Changes Still Present]
    
    F[Implementation Methods] --> G[Write-Ahead Logging]
    F --> H[Force-Write at Commit]
    F --> I[Redundant Storage]
    F --> J[Backup & Recovery]
    
    style E fill:#90EE90
    style G fill:#ADD8E6
    style H fill:#ADD8E6
    style I fill:#ADD8E6
    style J fill:#ADD8E6
```

---

## 🔄 Transaction States

Understanding the lifecycle of a transaction:

```mermaid
stateDiagram-v2
    [*] --> Active: BEGIN TRANSACTION
    Active --> PartiallyCommitted: COMMIT issued
    Active --> Failed: Error occurs
    PartiallyCommitted --> Committed: Write to disk complete
    PartiallyCommitted --> Failed: Write fails
    Failed --> Aborted: ROLLBACK complete
    Committed --> [*]: Transaction complete
    Aborted --> [*]: Transaction complete
    
    note right of Active: Executing operations
    note right of PartiallyCommitted: Waiting for disk write
    note right of Committed: Changes permanent
    note right of Aborted: Changes undone
```

### 📊 State Transition Examples

| State | Description | Example |
|-------|-------------|---------|
| **Active** | Transaction is executing | Running UPDATE statements |
| **Partially Committed** | Final statement executed, waiting for commit | COMMIT issued, disk write pending |
| **Committed** | Transaction completed successfully | Changes written to disk |
| **Failed** | Normal execution can no longer proceed | Constraint violation detected |
| **Aborted** | Transaction has been rolled back | All changes undone |

---

## 🚦 Concurrency Control

### 🎭 The Concurrency Challenge

When multiple transactions run simultaneously, various problems can occur:

```mermaid
graph TD
    A[Concurrency Problems] --> B[Lost Update Problem]
    A --> C[Dirty Read Problem]
    A --> D[Non-Repeatable Read]
    A --> E[Phantom Read Problem]
    
    B --> B1[T1 and T2 both update X]
    B --> B2[One update overwrites the other]
    
    C --> C1[T1 reads uncommitted data from T2]
    C --> C2[T2 later rolls back]
    
    D --> D1[T1 reads X twice]
    D --> D2[T2 modifies X between reads]
    
    E --> E1[T1 reads range of rows]
    E --> E2[T2 inserts new rows in range]
    
    style A fill:#FF6347
    style B fill:#FFB6C1
    style C fill:#FFB6C1
    style D fill:#FFB6C1
    style E fill:#FFB6C1
```

### 🔒 Locking Mechanisms

**Lock Types**:

```mermaid
graph TD
    A[Lock Types] --> B[Shared Lock S]
    A --> C[Exclusive Lock X]
    
    B --> B1[Multiple readers allowed]
    B --> B2[No writers allowed]
    
    C --> C1[Single writer only]
    C --> C2[No other readers or writers]
    
    D[Lock Compatibility Matrix]
    D --> E[S + S = ✅ Compatible]
    D --> F[S + X = ❌ Incompatible]
    D --> G[X + S = ❌ Incompatible]
    D --> H[X + X = ❌ Incompatible]
    
    style B fill:#90EE90
    style C fill:#FF6347
    style E fill:#90EE90
    style F fill:#FFB6C1
    style G fill:#FFB6C1
    style H fill:#FFB6C1
```

### 🔄 Two-Phase Locking (2PL)

**Protocol**: Ensures serializability by requiring transactions to acquire all locks before releasing any.

```mermaid
graph TD
    A[Two-Phase Locking] --> B[Growing Phase]
    A --> C[Shrinking Phase]
    
    B --> B1[Acquire locks only]
    B --> B2[No lock releases]
    
    C --> C1[Release locks only]
    C --> C2[No new lock acquisitions]
    
    D[Timeline Example] --> E[Acquire Lock A]
    E --> F[Acquire Lock B]
    F --> G[Lock Point - Switch to Shrinking]
    G --> H[Release Lock A]
    H --> I[Release Lock B]
    
    style B fill:#90EE90
    style C fill:#FFD700
    style G fill:#FF6347
```

**Advantages**: Guarantees serializability  
**Disadvantages**: Can cause deadlocks, reduced concurrency

---

## 🔍 Isolation Levels

SQL standard defines four isolation levels, each providing different trade-offs between consistency and performance:

```mermaid
graph TD
    A[Isolation Levels] --> B[Read Uncommitted]
    A --> C[Read Committed]
    A --> D[Repeatable Read]
    A --> E[Serializable]
    
    B --> B1[Allows: All anomalies]
    B --> B2[Performance: Highest]
    
    C --> C1[Prevents: Dirty reads]
    C --> C2[Allows: Non-repeatable, phantom reads]
    
    D --> D1[Prevents: Dirty, non-repeatable reads]
    D --> D2[Allows: Phantom reads]
    
    E --> E1[Prevents: All anomalies]
    E --> E2[Performance: Lowest]
    
    style B fill:#FFB6C1
    style C fill:#FFD700
    style D fill:#90EE90
    style E fill:#4169E1
```

### 📊 Isolation Level Comparison

| Isolation Level | Dirty Read | Non-Repeatable Read | Phantom Read | Performance |
|----------------|------------|-------------------|--------------|-------------|
| **Read Uncommitted** | ❌ Possible | ❌ Possible | ❌ Possible | 🚀 Highest |
| **Read Committed** | ✅ Prevented | ❌ Possible | ❌ Possible | 🏃 High |
| **Repeatable Read** | ✅ Prevented | ✅ Prevented | ❌ Possible | 🚶 Medium |
| **Serializable** | ✅ Prevented | ✅ Prevented | ✅ Prevented | 🐌 Lowest |

### 💡 Practical Usage Guidelines

```mermaid
graph TD
    A[Choose Isolation Level] --> B{Data Criticality?}
    B -->|High| C[Serializable/Repeatable Read]
    B -->|Medium| D[Read Committed]
    B -->|Low| E[Read Uncommitted]
    
    F{Concurrent Load?} --> G[High: Lower isolation]
    F --> H[Low: Higher isolation]
    
    style C fill:#90EE90
    style D fill:#FFD700
    style E fill:#FFB6C1
```

---

## 💾 Recovery Management

### 🔥 Failure Types

```mermaid
graph TD
    A[Database Failures] --> B[Transaction Failures]
    A --> C[System Failures]
    A --> D[Media Failures]
    
    B --> B1[Logical errors]
    B --> B2[Constraint violations]
    B --> B3[Deadlocks]
    
    C --> C1[Power outages]
    C --> C2[Software crashes]
    C --> C3[Operating system failures]
    
    D --> D1[Disk crashes]
    D --> D2[Tape failures]
    D --> D3[Catastrophic events]
    
    style B fill:#FFD700
    style C fill:#FF6347
    style D fill:#8B0000
```

### 📝 Write-Ahead Logging (WAL)

**Core Principle**: Log changes before applying them to the database.

```mermaid
sequenceDiagram
    participant T as Transaction
    participant L as Log
    participant D as Database
    
    T->>L: Write BEGIN record
    T->>L: Write UPDATE record (before image)
    T->>D: Update data page
    T->>L: Write COMMIT record
    L->>D: Force log to disk
    
    Note over L,D: Changes are recoverable
```

**Log Record Types**:
- **Start**: Transaction begins
- **Update**: Before and after images
- **Commit**: Transaction completes
- **Abort**: Transaction rolls back
- **Checkpoint**: Recovery point

### 🔄 Recovery Algorithms

#### ARIES (Algorithm for Recovery and Isolation Exploiting Semantics)

```mermaid
graph TD
    A[ARIES Recovery] --> B[Analysis Phase]
    A --> C[Redo Phase]
    A --> D[Undo Phase]
    
    B --> B1[Scan log from last checkpoint]
    B --> B2[Identify active transactions]
    B --> B3[Build dirty page table]
    
    C --> C1[Redo all logged operations]
    C --> C2[Restore database to failure point]
    
    D --> D1[Undo uncommitted transactions]
    D --> D2[Restore consistent state]
    
    style B fill:#ADD8E6
    style C fill:#90EE90
    style D fill:#FFD700
```

---

## 🛠️ SQL Transaction Control

### 📝 Basic Transaction Commands

```sql
-- Start a transaction
BEGIN TRANSACTION;
-- or
START TRANSACTION;

-- Commit changes
COMMIT;

-- Rollback changes
ROLLBACK;

-- Set savepoint
SAVEPOINT sp1;

-- Rollback to savepoint
ROLLBACK TO sp1;
```

### 🎯 Practical Examples

#### Example 1: Bank Transfer Transaction

```sql
BEGIN TRANSACTION;

-- Check account balances
SELECT balance FROM accounts WHERE account_id IN (101, 102);

-- Debit from account 101
UPDATE accounts 
SET balance = balance - 500 
WHERE account_id = 101 AND balance >= 500;

-- Check if debit succeeded
IF @@ROWCOUNT = 0
BEGIN
    ROLLBACK;
    RETURN;
END

-- Credit to account 102
UPDATE accounts 
SET balance = balance + 500 
WHERE account_id = 102;

-- Log the transaction
INSERT INTO transaction_log (from_account, to_account, amount, timestamp)
VALUES (101, 102, 500, GETDATE());

COMMIT TRANSACTION;
```

#### Example 2: Order Processing with Savepoints

```sql
BEGIN TRANSACTION;

-- Insert order
INSERT INTO orders (customer_id, order_date, total_amount)
VALUES (12345, GETDATE(), 150.00);

DECLARE @order_id INT = SCOPE_IDENTITY();

SAVEPOINT order_items;

-- Insert order items
INSERT INTO order_items (order_id, product_id, quantity, price)
VALUES (@order_id, 1001, 2, 50.00);

INSERT INTO order_items (order_id, product_id, quantity, price)
VALUES (@order_id, 1002, 1, 50.00);

-- Update inventory
UPDATE products 
SET stock_quantity = stock_quantity - 2 
WHERE product_id = 1001 AND stock_quantity >= 2;

IF @@ROWCOUNT = 0
BEGIN
    ROLLBACK TO order_items;
    -- Could partially fulfill order or cancel completely
    ROLLBACK;
    RETURN;
END

UPDATE products 
SET stock_quantity = stock_quantity - 1 
WHERE product_id = 1002 AND stock_quantity >= 1;

COMMIT TRANSACTION;
```

### ⚙️ Setting Isolation Levels

```sql
-- Set isolation level for current session
SET TRANSACTION ISOLATION LEVEL READ COMMITTED;

-- Set for specific transaction
BEGIN TRANSACTION;
SET TRANSACTION ISOLATION LEVEL REPEATABLE READ;
-- ... transaction operations ...
COMMIT;

-- Table hints for specific queries
SELECT * FROM products WITH (NOLOCK);  -- READ UNCOMMITTED
SELECT * FROM products WITH (HOLDLOCK); -- REPEATABLE READ
```

---

## 🧠 Advanced Concepts

### ⚡ Optimistic vs Pessimistic Concurrency Control

```mermaid
graph LR
    A[Concurrency Control Strategies] --> B[Pessimistic]
    A --> C[Optimistic]
    
    B --> B1[Assume conflicts will occur]
    B --> B2[Lock early and often]
    B --> B3[Better for high contention]
    
    C --> C1[Assume conflicts are rare]
    C --> C2[Check for conflicts at commit]
    C --> C3[Better for low contention]
    
    style B fill:#FFB6C1
    style C fill:#90EE90
```

**Optimistic Locking Example**:
```sql
-- Read with version number
SELECT id, name, version FROM employees WHERE id = 100;
-- version = 42

-- Later, update with version check
UPDATE employees 
SET name = 'John Smith', version = version + 1
WHERE id = 100 AND version = 42;

-- If @@ROWCOUNT = 0, someone else modified the record
```

### 🕸️ Deadlock Detection and Prevention

```mermaid
graph TD
    A[Deadlock Scenarios] --> B[T1 waits for T2]
    A --> C[T2 waits for T1]
    B --> D[Circular Wait = Deadlock]
    C --> D
    
    E[Detection Methods] --> F[Wait-for Graph]
    E --> G[Timeout-based]
    E --> H[Victim Selection]
    
    I[Prevention Strategies] --> J[Ordered Locking]
    I --> K[Timeout Limits]
    I --> L[Deadlock Priority]
    
    style D fill:#FF6347
    style F fill:#ADD8E6
    style G fill:#ADD8E6
    style H fill:#ADD8E6
```

---

## 🧪 Interactive Scenarios

### 🎮 Scenario 1: Lost Update Problem

**Setup**: Two cashiers processing returns simultaneously

```mermaid
sequenceDiagram
    participant C1 as Cashier 1
    participant C2 as Cashier 2
    participant DB as Database
    
    DB->>C1: Read inventory: 100 units
    DB->>C2: Read inventory: 100 units
    C1->>DB: Return 5 units (100 + 5 = 105)
    C2->>DB: Return 3 units (100 + 3 = 103)
    
    Note over DB: Final value: 103 ❌<br/>Should be: 108 ✅
```

**Question**: How would you prevent this lost update problem?

<details>
<summary>💡 Solution</summary>

**Solutions**:
1. **Locking**: Use exclusive locks on inventory records
2. **Optimistic Concurrency**: Use version numbers or timestamps
3. **Atomic Operations**: Use SQL UPDATE with expressions

```sql
-- Solution 1: Explicit Locking
BEGIN TRANSACTION;
SELECT inventory_count FROM products WITH (UPDLOCK) WHERE id = 1001;
UPDATE products SET inventory_count = inventory_count + 5 WHERE id = 1001;
COMMIT;

-- Solution 2: Atomic Update
UPDATE products 
SET inventory_count = inventory_count + 5 
WHERE id = 1001;
```
</details>

### 🎮 Scenario 2: Phantom Read Detection

**Setup**: Manager generating reports while new employees are being added

<details>
<summary>💭 Think About It</summary>

At REPEATABLE READ isolation level:
- The same SELECT will return the same rows
- But new rows can still appear (phantoms)
- This affects aggregate functions like COUNT, SUM

**Question**: What isolation level would prevent phantom reads?
</details>

---

## 📊 Performance Considerations

### 🚀 Transaction Optimization Tips

```mermaid
graph TD
    A[Transaction Performance] --> B[Keep Transactions Short]
    A --> C[Minimize Lock Duration]
    A --> D[Use Appropriate Isolation]
    A --> E[Batch Operations]
    
    B --> B1[Faster commit times]
    B --> B2[Reduced lock contention]
    
    C --> C1[Acquire locks late]
    C --> C2[Release locks early]
    
    D --> D1[Balance consistency vs performance]
    D --> D2[Consider read patterns]
    
    E --> E1[Reduce transaction overhead]
    E --> E2[Improve throughput]
    
    style A fill:#4169E1
    style B1 fill:#90EE90
    style B2 fill:#90EE90
    style C1 fill:#90EE90
    style C2 fill:#90EE90
```

### 📈 Monitoring Transaction Health

**Key Metrics to Track**:
- **Transaction Duration**: Average and maximum time
- **Lock Wait Time**: Time spent waiting for locks
- **Deadlock Frequency**: Number of deadlocks per hour
- **Rollback Ratio**: Percentage of transactions rolled back
- **Log Growth Rate**: Size of transaction log

---

## 🧪 Hands-On Lab Exercise

### 🔬 Transaction Isolation Experiment

**Objective**: Observe different isolation levels in action

**Setup**:
```sql
-- Create test table
CREATE TABLE accounts (
    id INT PRIMARY KEY,
    name VARCHAR(50),
    balance DECIMAL(10,2)
);

INSERT INTO accounts VALUES 
(1, 'Alice', 1000.00),
(2, 'Bob', 500.00);
```

**Experiment Steps**:

1. **Session 1** (READ COMMITTED):
```sql
SET TRANSACTION ISOLATION LEVEL READ COMMITTED;
BEGIN TRANSACTION;
SELECT balance FROM accounts WHERE id = 1;
-- Wait for Session 2 to make changes
SELECT balance FROM accounts WHERE id = 1;
COMMIT;
```

2. **Session 2**:
```sql
BEGIN TRANSACTION;
UPDATE accounts SET balance = 1500.00 WHERE id = 1;
-- Session 1 will see the old value until we commit
COMMIT;
```

**Expected Results**:
- Session 1 will see different values in the two SELECT statements
- This demonstrates non-repeatable read

**Try with REPEATABLE READ**: What changes?

---

## 🔗 Key Takeaways

```mermaid
mindmap
  root((Transaction Management))
    ACID Properties
      Atomicity
      Consistency  
      Isolation
      Durability
    Concurrency Control
      Locking
      Isolation Levels
      Deadlock Handling
    Recovery
      Logging
      Checkpoint
      Rollback
    Best Practices
      Short Transactions
      Appropriate Isolation
      Error Handling
      Performance Monitoring
```

### 🎯 Essential Principles

1. **🛡️ ACID is non-negotiable** for critical systems
2. **⚖️ Balance consistency with performance** using appropriate isolation levels  
3. **🔒 Understand your locking strategy** to avoid deadlocks
4. **📝 Always handle transaction failures** gracefully
5. **📊 Monitor transaction performance** regularly
6. **🔄 Design for recovery** from day one

### 🚀 Next Steps

- Practice implementing transactions in your database system
- Experiment with different isolation levels
- Set up monitoring for transaction metrics
- Design backup and recovery procedures
- Learn about distributed transaction management (2PC, Saga patterns)

---
# 🔧 Advanced SQL Operations

> **"Advanced SQL is where data transforms from information into intelligence"**

## 🎯 Chapter Overview

Moving beyond basic CRUD operations, advanced SQL techniques unlock the full potential of relational databases. From complex analytical queries to performance optimization strategies, this section covers the sophisticated tools that separate SQL novices from database professionals.

```mermaid
graph TD
    A[Advanced SQL Mastery] --> B[Complex Queries]
    A --> C[Window Functions]
    A --> D[Common Table Expressions]
    A --> E[Stored Procedures]
    A --> F[Performance Optimization]
    
    B --> B1[Subqueries & Joins]
    B --> B2[Set Operations]
    B --> B3[Conditional Logic]
    
    C --> C1[Ranking Functions]
    C --> C2[Aggregate Functions]
    C --> C3[Value Functions]
    
    D --> D1[Recursive CTEs]
    D --> D2[Multiple CTEs]
    D --> D3[Query Organization]
    
    E --> E1[Functions]
    E --> E2[Procedures]
    E --> E3[Triggers]
    
    F --> F1[Index Strategy]
    F --> F2[Query Plans]
    F --> F3[Statistics]
    
    style A fill:#4169E1
    style B fill:#32CD32
    style C fill:#FF6347
    style D fill:#FFD700
    style E fill:#9370DB
    style F fill:#FF1493
```

### 🎓 Learning Objectives

By the end of this section, you will be able to:

✅ **Write** complex analytical queries using advanced SQL features  
✅ **Apply** window functions for sophisticated data analysis  
✅ **Design** recursive queries for hierarchical data  
✅ **Optimize** query performance using various techniques  
✅ **Create** reusable database objects like functions and procedures  
✅ **Analyze** query execution plans for performance tuning  

---

## 🔍 Advanced Query Techniques

### 🎯 Subqueries vs Joins Performance

Understanding when to use subqueries vs joins is crucial for optimal performance:

```mermaid
graph LR
    A[Query Strategy Decision] --> B{Data Selectivity?}
    B -->|High Selectivity| C[Subquery Often Better]
    B -->|Low Selectivity| D[Join Often Better]
    
    E{Result Set Size?} --> F[Small Result: Subquery]
    E --> G[Large Result: Join]
    
    H{Index Availability?} --> I[Good Indexes: Join]
    H --> J[Poor Indexes: Consider Subquery]
    
    style C fill:#90EE90
    style D fill:#90EE90
    style F fill:#FFD700
    style G fill:#FFD700
```

#### 📊 Correlated vs Non-Correlated Subqueries

**Non-Correlated Subquery** (executed once):
```sql
-- Find employees earning more than average salary
SELECT employee_id, name, salary
FROM employees
WHERE salary > (
    SELECT AVG(salary) 
    FROM employees
);
```

**Correlated Subquery** (executed for each outer row):
```sql
-- Find employees earning more than department average
SELECT e1.employee_id, e1.name, e1.salary, e1.department_id
FROM employees e1
WHERE e1.salary > (
    SELECT AVG(e2.salary)
    FROM employees e2
    WHERE e2.department_id = e1.department_id
);
```

### 🔄 Advanced Join Patterns

#### 🎭 Self-Joins for Hierarchical Data

```sql
-- Find employees and their managers
SELECT 
    e.name AS employee_name,
    m.name AS manager_name,
    e.salary,
    m.salary AS manager_salary
FROM employees e
LEFT JOIN employees m ON e.manager_id = m.employee_id;

-- Find employees earning more than their managers
SELECT 
    e.name AS employee_name,
    e.salary AS employee_salary,
    m.name AS manager_name,
    m.salary AS manager_salary
FROM employees e
INNER JOIN employees m ON e.manager_id = m.employee_id
WHERE e.salary > m.salary;
```

#### 🔀 Cross Apply vs Outer Apply (SQL Server)

```sql
-- Cross Apply: Inner join behavior with table-valued functions
SELECT 
    d.department_name,
    t.employee_name,
    t.salary
FROM departments d
CROSS APPLY (
    SELECT TOP 3 name AS employee_name, salary
    FROM employees e
    WHERE e.department_id = d.department_id
    ORDER BY salary DESC
) t;

-- Outer Apply: Left join behavior with table-valued functions
SELECT 
    d.department_name,
    t.employee_name,
    t.salary
FROM departments d
OUTER APPLY (
    SELECT TOP 1 name AS employee_name, salary
    FROM employees e
    WHERE e.department_id = d.department_id
    ORDER BY salary DESC
) t;
```

---

## 🪟 Window Functions Mastery

Window functions are among the most powerful features in modern SQL, enabling complex analytical operations:

```mermaid
graph TD
    A[Window Functions] --> B[Ranking Functions]
    A --> C[Aggregate Functions]
    A --> D[Value Functions]
    
    B --> B1[ROW_NUMBER]
    B --> B2[RANK]
    B --> B3[DENSE_RANK]
    B --> B4[NTILE]
    
    C --> C1[SUM OVER]
    C --> C2[AVG OVER]
    C --> C3[COUNT OVER]
    C --> C4[MIN/MAX OVER]
    
    D --> D1[LAG/LEAD]
    D --> D2[FIRST_VALUE]
    D --> D3[LAST_VALUE]
    D --> D4[NTH_VALUE]
    
    style A fill:#4169E1
    style B fill:#FF6347
    style C fill:#32CD32
    style D fill:#FFD700
```

### 🥇 Ranking Functions

```sql
-- Comprehensive ranking example
SELECT 
    department_id,
    name,
    salary,
    -- Different ranking methods
    ROW_NUMBER() OVER (PARTITION BY department_id ORDER BY salary DESC) as row_num,
    RANK() OVER (PARTITION BY department_id ORDER BY salary DESC) as rank_pos,
    DENSE_RANK() OVER (PARTITION BY department_id ORDER BY salary DESC) as dense_rank_pos,
    NTILE(4) OVER (PARTITION BY department_id ORDER BY salary DESC) as quartile,
    
    -- Percentage calculations
    PERCENT_RANK() OVER (PARTITION BY department_id ORDER BY salary) as percent_rank,
    CUME_DIST() OVER (PARTITION BY department_id ORDER BY salary) as cumulative_dist
FROM employees
ORDER BY department_id, salary DESC;
```

**Output Visualization**:

| Department | Name | Salary | ROW_NUMBER | RANK | DENSE_RANK | NTILE |
|------------|------|--------|------------|------|------------|-------|
| IT | Alice | 90000 | 1 | 1 | 1 | 1 |
| IT | Bob | 85000 | 2 | 2 | 2 | 1 |
| IT | Carol | 85000 | 3 | 2 | 2 | 2 |
| IT | Dave | 75000 | 4 | 4 | 3 | 2 |

### 📊 Running Totals and Moving Averages

```sql
-- Advanced aggregation with window frames
SELECT 
    order_date,
    daily_sales,
    
    -- Running total
    SUM(daily_sales) OVER (
        ORDER BY order_date 
        ROWS UNBOUNDED PRECEDING
    ) as running_total,
    
    -- 7-day moving average
    AVG(daily_sales) OVER (
        ORDER BY order_date 
        ROWS 6 PRECEDING
    ) as moving_avg_7day,
    
    -- Month-to-date total
    SUM(daily_sales) OVER (
        PARTITION BY YEAR(order_date), MONTH(order_date)
        ORDER BY order_date
        ROWS UNBOUNDED PRECEDING
    ) as mtd_total,
    
    -- Compare with previous day
    daily_sales - LAG(daily_sales, 1) OVER (ORDER BY order_date) as daily_change,
    
    -- Compare with same day last week
    daily_sales - LAG(daily_sales, 7) OVER (ORDER BY order_date) as weekly_change
    
FROM daily_sales_summary
ORDER BY order_date;
```

### 🔄 Lead and Lag Functions

```sql
-- Time series analysis with LAG and LEAD
SELECT 
    customer_id,
    order_date,
    order_amount,
    
    -- Previous order information
    LAG(order_date, 1) OVER (
        PARTITION BY customer_id 
        ORDER BY order_date
    ) as previous_order_date,
    
    LAG(order_amount, 1) OVER (
        PARTITION BY customer_id 
        ORDER BY order_date
    ) as previous_order_amount,
    
    -- Next order information
    LEAD(order_date, 1) OVER (
        PARTITION BY customer_id 
        ORDER BY order_date
    ) as next_order_date,
    
    -- Days between orders
    DATEDIFF(day, 
        LAG(order_date, 1) OVER (
            PARTITION BY customer_id 
            ORDER BY order_date
        ), 
        order_date
    ) as days_since_last_order,
    
    -- Customer's first and last order amounts
    FIRST_VALUE(order_amount) OVER (
        PARTITION BY customer_id 
        ORDER BY order_date
        ROWS UNBOUNDED PRECEDING
    ) as first_order_amount,
    
    LAST_VALUE(order_amount) OVER (
        PARTITION BY customer_id 
        ORDER BY order_date
        ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING
    ) as last_order_amount

FROM orders
ORDER BY customer_id, order_date;
```

---

## 🌳 Common Table Expressions (CTEs)

CTEs provide a way to create temporary, named result sets that exist only for the duration of a single query:

### 🔄 Recursive CTEs

Perfect for hierarchical data like organizational charts, bill of materials, or category trees:

```sql
-- Organizational hierarchy traversal
WITH EmployeeHierarchy AS (
    -- Anchor: Find top-level managers
    SELECT 
        employee_id,
        name,
        manager_id,
        title,
        0 as level,
        CAST(name AS VARCHAR(1000)) as hierarchy_path
    FROM employees
    WHERE manager_id IS NULL
    
    UNION ALL
    
    -- Recursive: Find all subordinates
    SELECT 
        e.employee_id,
        e.name,
        e.manager_id,
        e.title,
        eh.level + 1,
        CAST(eh.hierarchy_path + ' > ' + e.name AS VARCHAR(1000))
    FROM employees e
    INNER JOIN EmployeeHierarchy eh ON e.manager_id = eh.employee_id
)
SELECT 
    employee_id,
    REPLICATE('  ', level) + name as indented_name,
    title,
    level,
    hierarchy_path
FROM EmployeeHierarchy
ORDER BY hierarchy_path;
```

### 🔢 Multiple CTEs for Complex Analysis

```sql
-- Sales analysis with multiple CTEs
WITH MonthlySales AS (
    SELECT 
        YEAR(order_date) as year,
        MONTH(order_date) as month,
        SUM(total_amount) as monthly_total,
        COUNT(*) as order_count
    FROM orders
    GROUP BY YEAR(order_date), MONTH(order_date)
),
SalesGrowth AS (
    SELECT 
        year,
        month,
        monthly_total,
        order_count,
        LAG(monthly_total, 1) OVER (ORDER BY year, month) as prev_month_total,
        LAG(monthly_total, 12) OVER (ORDER BY year, month) as same_month_prev_year
    FROM MonthlySales
),
SalesMetrics AS (
    SELECT 
        year,
        month,
        monthly_total,
        order_count,
        -- Month-over-month growth
        CASE 
            WHEN prev_month_total IS NOT NULL 
            THEN (monthly_total - prev_month_total) / prev_month_total * 100 
        END as mom_growth_pct,
        -- Year-over-year growth
        CASE 
            WHEN same_month_prev_year IS NOT NULL 
            THEN (monthly_total - same_month_prev_year) / same_month_prev_year * 100 
        END as yoy_growth_pct
    FROM SalesGrowth
)
SELECT 
    year,
    month,
    monthly_total,
    order_count,
    ROUND(mom_growth_pct, 2) as mom_growth_pct,
    ROUND(yoy_growth_pct, 2) as yoy_growth_pct,
    -- Categorize performance
    CASE 
        WHEN mom_growth_pct > 10 THEN 'Excellent Growth'
        WHEN mom_growth_pct > 0 THEN 'Positive Growth'
        WHEN mom_growth_pct > -10 THEN 'Slight Decline'
        ELSE 'Significant Decline'
    END as performance_category
FROM SalesMetrics
ORDER BY year, month;
```

---

## 🎨 Advanced Data Manipulation

### 🔄 MERGE Statement (Upsert Operations)

```sql
-- Sophisticated data synchronization
MERGE target_customers AS target
USING (
    SELECT 
        customer_id,
        name,
        email,
        last_updated,
        status
    FROM source_customers
    WHERE last_updated > '2023-01-01'
) AS source ON target.customer_id = source.customer_id

WHEN MATCHED AND source.last_updated > target.last_updated THEN
    UPDATE SET
        name = source.name,
        email = source.email,
        last_updated = source.last_updated,
        status = source.status,
        updated_at = GETDATE()

WHEN NOT MATCHED BY TARGET THEN
    INSERT (customer_id, name, email, last_updated, status, created_at)
    VALUES (source.customer_id, source.name, source.email, 
            source.last_updated, source.status, GETDATE())

WHEN NOT MATCHED BY SOURCE AND target.status = 'ACTIVE' THEN
    UPDATE SET 
        status = 'INACTIVE',
        updated_at = GETDATE()

OUTPUT 
    $action as operation,
    inserted.customer_id as affected_customer_id,
    inserted.name as customer_name;
```

### 🎯 Conditional Aggregation

```sql
-- Advanced pivot-like operations without PIVOT
SELECT 
    department_id,
    department_name,
    
    -- Conditional counts
    COUNT(*) as total_employees,
    SUM(CASE WHEN salary >= 100000 THEN 1 ELSE 0 END) as high_earners,
    SUM(CASE WHEN salary < 50000 THEN 1 ELSE 0 END) as low_earners,
    
    -- Conditional averages
    AVG(CASE WHEN title LIKE '%Manager%' THEN salary END) as avg_manager_salary,
    AVG(CASE WHEN title NOT LIKE '%Manager%' THEN salary END) as avg_staff_salary,
    
    -- Percentage calculations
    ROUND(
        100.0 * SUM(CASE WHEN salary >= 100000 THEN 1 ELSE 0 END) / COUNT(*), 
        2
    ) as high_earner_percentage,
    
    -- Multiple conditions
    SUM(CASE 
        WHEN salary >= 100000 AND DATEDIFF(year, hire_date, GETDATE()) > 5 
        THEN 1 ELSE 0 
    END) as senior_high_earners

FROM employees e
JOIN departments d ON e.department_id = d.department_id
GROUP BY department_id, department_name
HAVING COUNT(*) > 5  -- Only departments with more than 5 employees
ORDER BY high_earner_percentage DESC;
```

---

## 🏗️ Stored Procedures and Functions

### 📝 Advanced Stored Procedures

```sql
-- Comprehensive stored procedure with error handling
CREATE PROCEDURE sp_ProcessCustomerOrder
    @customer_id INT,
    @product_details NVARCHAR(MAX), -- JSON format
    @discount_percent DECIMAL(5,2) = 0,
    @order_id INT OUTPUT,
    @total_amount DECIMAL(10,2) OUTPUT
AS
BEGIN
    SET NOCOUNT ON;
    
    DECLARE @error_message NVARCHAR(1000);
    DECLARE @temp_order_id INT;
    
    BEGIN TRY
        BEGIN TRANSACTION;
        
        -- Validate customer
        IF NOT EXISTS (SELECT 1 FROM customers WHERE customer_id = @customer_id AND status = 'ACTIVE')
        BEGIN
            RAISERROR('Invalid or inactive customer', 16, 1);
            RETURN;
        END
        
        -- Create order header
        INSERT INTO orders (customer_id, order_date, status, discount_percent)
        VALUES (@customer_id, GETDATE(), 'PENDING', @discount_percent);
        
        SET @temp_order_id = SCOPE_IDENTITY();
        
        -- Parse JSON and insert order items
        INSERT INTO order_items (order_id, product_id, quantity, price)
        SELECT 
            @temp_order_id,
            JSON_VALUE(value, '$.product_id'),
            JSON_VALUE(value, '$.quantity'),
            p.price
        FROM OPENJSON(@product_details) 
        CROSS JOIN products p 
        WHERE p.product_id = JSON_VALUE(value, '$.product_id')
        AND p.stock_quantity >= CAST(JSON_VALUE(value, '$.quantity') AS INT);
        
        -- Check if all items were inserted
        IF @@ROWCOUNT = 0
        BEGIN
            RAISERROR('No valid products found or insufficient stock', 16, 1);
            RETURN;
        END
        
        -- Update inventory
        UPDATE p
        SET stock_quantity = stock_quantity - CAST(JSON_VALUE(od.value, '$.quantity') AS INT),
            last_updated = GETDATE()
        FROM products p
        INNER JOIN OPENJSON(@product_details) od ON p.product_id = JSON_VALUE(od.value, '$.product_id');
        
        -- Calculate total
        SELECT @total_amount = SUM(quantity * unit_price * (1 - @discount_percent / 100))
        FROM order_items
        WHERE order_id = @temp_order_id;
        
        -- Update order total
        UPDATE orders 
        SET total_amount = @total_amount,
            status = 'CONFIRMED'
        WHERE order_id = @temp_order_id;
        
        SET @order_id = @temp_order_id;
        
        COMMIT TRANSACTION;
        
        -- Success notification
        PRINT 'Order processed successfully. Order ID: ' + CAST(@order_id AS VARCHAR(10));
        
    END TRY
    BEGIN CATCH
        IF @@TRANCOUNT > 0
            ROLLBACK TRANSACTION;
            
        SET @error_message = ERROR_MESSAGE();
        
        -- Log error
        INSERT INTO error_log (procedure_name, error_message, error_date, parameters)
        VALUES ('sp_ProcessCustomerOrder', @error_message, GETDATE(), 
                'customer_id=' + CAST(@customer_id AS VARCHAR(10)));
        
        -- Re-raise error
        RAISERROR(@error_message, 16, 1);
    END CATCH
END
```

### 🧮 Table-Valued Functions

```sql
-- Inline Table-Valued Function
CREATE FUNCTION fn_GetEmployeeSalesHistory(@employee_id INT, @start_date DATE, @end_date DATE)
RETURNS TABLE
AS
RETURN
(
    SELECT 
        s.sale_date,
        s.amount,
        s.commission,
        c.name as customer_name,
        p.name as product_name,
        RANK() OVER (ORDER BY s.amount DESC) as sale_rank
    FROM sales s
    JOIN customers c ON s.customer_id = c.customer_id
    JOIN products p ON s.product_id = p.product_id
    WHERE s.employee_id = @employee_id
    AND s.sale_date BETWEEN @start_date AND @end_date
);

-- Multi-Statement Table-Valued Function
CREATE FUNCTION fn_CalculateCommissionTiers(@department_id INT)
RETURNS @CommissionTable TABLE (
    employee_id INT,
    employee_name VARCHAR(100),
    total_sales DECIMAL(12,2),
    commission_rate DECIMAL(5,4),
    commission_amount DECIMAL(10,2),
    tier_level VARCHAR(20)
)
AS
BEGIN
    INSERT INTO @CommissionTable
    SELECT 
        e.employee_id,
        e.name,
        ISNULL(SUM(s.amount), 0) as total_sales,
        CASE 
            WHEN SUM(s.amount) >= 100000 THEN 0.15
            WHEN SUM(s.amount) >= 50000 THEN 0.10
            WHEN SUM(s.amount) >= 25000 THEN 0.05
            ELSE 0.02
        END as commission_rate,
        CASE 
            WHEN SUM(s.amount) >= 100000 THEN SUM(s.amount) * 0.15
            WHEN SUM(s.amount) >= 50000 THEN SUM(s.amount) * 0.10
            WHEN SUM(s.amount) >= 25000 THEN SUM(s.amount) * 0.05
            ELSE SUM(s.amount) * 0.02
        END as commission_amount,
        CASE 
            WHEN SUM(s.amount) >= 100000 THEN 'Platinum'
            WHEN SUM(s.amount) >= 50000 THEN 'Gold'
            WHEN SUM(s.amount) >= 25000 THEN 'Silver'
            ELSE 'Bronze'
        END as tier_level
    FROM employees e
    LEFT JOIN sales s ON e.employee_id = s.employee_id
    WHERE e.department_id = @department_id
    GROUP BY e.employee_id, e.name;
    
    RETURN;
END
```

---

## ⚡ Query Performance Optimization

### 📊 Understanding Query Execution Plans

```mermaid
graph TD
    A[Query Execution Plan] --> B[Scan Operations]
    A --> C[Seek Operations]
    A --> D[Join Algorithms]
    A --> E[Sort Operations]
    
    B --> B1[Table Scan - Expensive]
    B --> B2[Index Scan - Better]
    
    C --> C1[Index Seek - Best]
    C --> C2[Key Lookup - Costly]
    
    D --> D1[Nested Loop Join]
   
    D --> D2[Hash Join]
    D --> D3[Merge Join]
    
    E --> E1[Sort in tempdb]
    E --> E2[Sort avoided by index]
    
    style B1 fill:#FFB6C1
    style B2 fill:#FFD700
    style C1 fill:#90EE90
    style C2 fill:#FFB6C1
    style E2 fill:#90EE90
```

### 🎯 Index Strategy

```sql
-- Composite index design
CREATE NONCLUSTERED INDEX IX_Orders_CustomerDate_Includes
ON orders (customer_id, order_date)
INCLUDE (total_amount, status, discount_percent);

-- Filtered index for active records
CREATE NONCLUSTERED INDEX IX_Employees_Active_Salary
ON employees (department_id, salary)
WHERE status = 'ACTIVE';

-- Covering index for specific query patterns
CREATE NONCLUSTERED INDEX IX_Sales_CoveringIndex
ON sales (sale_date, employee_id)
INCLUDE (customer_id, product_id, amount, commission);
```

### 🔍 Query Optimization Techniques

```sql
-- Original slow query (avoid this)
SELECT DISTINCT 
    c.name,
    (SELECT COUNT(*) FROM orders o WHERE o.customer_id = c.customer_id) as order_count,
    (SELECT SUM(total_amount) FROM orders o WHERE o.customer_id = c.customer_id) as total_spent
FROM customers c
WHERE c.registration_date > '2023-01-01'
ORDER BY c.name;

-- Optimized version
WITH CustomerMetrics AS (
    SELECT 
        c.customer_id,
        c.name,
        c.registration_date,
        COUNT(o.order_id) as order_count,
        SUM(o.total_amount) as total_spent
    FROM customers c
    LEFT JOIN orders o ON c.customer_id = o.customer_id
    WHERE c.registration_date > '2023-01-01'
    GROUP BY c.customer_id, c.name, c.registration_date
)
SELECT 
    name,
    order_count,
    ISNULL(total_spent, 0) as total_spent
FROM CustomerMetrics
ORDER BY name;

-- Using EXISTS instead of IN for better performance
SELECT customer_id, name, email
FROM customers c
WHERE EXISTS (
    SELECT 1 
    FROM orders o 
    WHERE o.customer_id = c.customer_id 
    AND o.order_date >= '2023-01-01'
);

-- Avoid functions in WHERE clause
-- Instead of: WHERE YEAR(order_date) = 2023
-- Use: WHERE order_date >= '2023-01-01' AND order_date < '2024-01-01'
```

---

## 🧪 Advanced Analytics Examples

### 📈 Customer Cohort Analysis

```sql
-- Cohort analysis to track customer retention
WITH CustomerCohorts AS (
    SELECT 
        customer_id,
        MIN(order_date) as first_order_date,
        DATEFROMPARTS(YEAR(MIN(order_date)), MONTH(MIN(order_date)), 1) as cohort_month
    FROM orders
    GROUP BY customer_id
),
CustomerOrders AS (
    SELECT 
        o.customer_id,
        o.order_date,
        cc.cohort_month,
        DATEDIFF(month, cc.cohort_month, 
                DATEFROMPARTS(YEAR(o.order_date), MONTH(o.order_date), 1)) as period_number
    FROM orders o
    JOIN CustomerCohorts cc ON o.customer_id = cc.customer_id
),
CohortData AS (
    SELECT 
        cohort_month,
        period_number,
        COUNT(DISTINCT customer_id) as customers
    FROM CustomerOrders
    GROUP BY cohort_month, period_number
),
CohortSizes AS (
    SELECT 
        cohort_month,
        customers as cohort_size
    FROM CohortData
    WHERE period_number = 0
)
SELECT 
    cd.cohort_month,
    cs.cohort_size,
    cd.period_number,
    cd.customers,
    ROUND(100.0 * cd.customers / cs.cohort_size, 2) as retention_rate
FROM CohortData cd
JOIN CohortSizes cs ON cd.cohort_month = cs.cohort_month
ORDER BY cd.cohort_month, cd.period_number;
```

### 🎯 RFM Analysis (Recency, Frequency, Monetary)

```sql
-- RFM Analysis for customer segmentation
WITH CustomerRFM AS (
    SELECT 
        customer_id,
        DATEDIFF(day, MAX(order_date), GETDATE()) as recency_days,
        COUNT(*) as frequency,
        SUM(total_amount) as monetary_value
    FROM orders
    WHERE order_date >= DATEADD(year, -1, GETDATE())
    GROUP BY customer_id
),
RFMScores AS (
    SELECT 
        customer_id,
        recency_days,
        frequency,
        monetary_value,
        NTILE(5) OVER (ORDER BY recency_days DESC) as recency_score,
        NTILE(5) OVER (ORDER BY frequency ASC) as frequency_score,
        NTILE(5) OVER (ORDER BY monetary_value ASC) as monetary_score
    FROM CustomerRFM
),
CustomerSegments AS (
    SELECT 
        customer_id,
        recency_score,
        frequency_score,
        monetary_score,
        CASE 
            WHEN recency_score >= 4 AND frequency_score >= 4 AND monetary_score >= 4 
            THEN 'Champions'
            WHEN recency_score >= 3 AND frequency_score >= 3 AND monetary_score >= 3 
            THEN 'Loyal Customers'
            WHEN recency_score >= 4 AND frequency_score <= 2 
            THEN 'New Customers'
            WHEN recency_score <= 2 AND frequency_score >= 3 
            THEN 'At Risk'
            WHEN recency_score <= 2 AND frequency_score <= 2 
            THEN 'Lost Customers'
            ELSE 'Potential Loyalists'
        END as customer_segment
    FROM RFMScores
)
SELECT 
    customer_segment,
    COUNT(*) as customer_count,
    AVG(recency_score) as avg_recency,
    AVG(frequency_score) as avg_frequency,
    AVG(monetary_score) as avg_monetary
FROM CustomerSegments
GROUP BY customer_segment
ORDER BY customer_count DESC;
```

---

## 🧪 Interactive Challenge

### 🎮 Query Optimization Challenge

**Scenario**: You have a slow-running report query that times out during peak hours.

**Original Query**:
```sql
SELECT DISTINCT 
    c.name,
    (SELECT COUNT(*) FROM orders o WHERE o.customer_id = c.customer_id) as order_count,
    (SELECT SUM(total_amount) FROM orders o WHERE o.customer_id = c.customer_id) as total_spent
FROM customers c
WHERE c.registration_date > '2023-01-01'
ORDER BY c.name;

-- Optimized version
WITH CustomerMetrics AS (
    SELECT 
        c.customer_id,
        c.name,
        c.registration_date,
        COUNT(o.order_id) as order_count,
        SUM(o.total_amount) as total_spent
    FROM customers c
    LEFT JOIN orders o ON c.customer_id = o.customer_id
    WHERE c.registration_date > '2023-01-01'
    GROUP BY c.customer_id, c.name, c.registration_date
)
SELECT 
    name,
    order_count,
    ISNULL(total_spent, 0) as total_spent
FROM CustomerMetrics
ORDER BY name;

-- Using EXISTS instead of IN for better performance
SELECT customer_id, name, email
FROM customers c
WHERE EXISTS (
    SELECT 1 
    FROM orders o 
    WHERE o.customer_id = c.customer_id 
    AND o.order_date >= '2023-01-01'
);

-- Avoid functions in WHERE clause
-- Instead of: WHERE YEAR(order_date) = 2023
-- Use: WHERE order_date >= '2023-01-01' AND order_date < '2024-01-01'
```

**Challenge Questions**:
1. What are the potential performance issues?
2. How would you optimize this query?
3. What indexes would you recommend?

<details>
<summary>💡 Solution</summary>

**Performance Issues**:
- Function in WHERE clause (`YEAR(o.order_date)`)
- Subquery could be replaced with JOIN
- No obvious indexes mentioned

**Optimized Query**:
```sql
-- Create supporting indexes first
CREATE INDEX IX_Orders_Date_Includes ON orders (order_date) INCLUDE (order_id);
CREATE INDEX IX_Categories_Name ON categories (name) WHERE name LIKE '%Electronics%';

-- Optimized query
WITH ElectronicsCategories AS (
    SELECT category_id 
    FROM categories 
    WHERE name LIKE '%Electronics%'
)
SELECT 
    p.name as product_name,
    COUNT(*) as times_ordered,
    AVG(oi.unit_price) as avg_price,
    SUM(oi.quantity * oi.unit_price) as total_revenue
FROM products p
JOIN ElectronicsCategories ec ON p.category_id = ec.category_id
JOIN order_items oi ON p.product_id = oi.product_id
JOIN orders o ON oi.order_id = o.order_id
WHERE o.order_date >= '2023-01-01' 
    AND o.order_date < '2024-01-01'
GROUP BY p.product_id, p.name
HAVING COUNT(*) > 10
ORDER BY total_revenue DESC;
```

**Key Improvements**:
- Replaced `YEAR()` function with date range
- Converted subquery to CTE and JOIN
- Added appropriate indexes
</details>

---
